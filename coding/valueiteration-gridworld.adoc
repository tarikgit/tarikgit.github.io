= Value Iteration - Gridworld
:stem:

We consider a rectangular gridworld representation (see below) of a simple finite Markov Decision Process (MDP).
The cells of the grid correspond to the states of the environment.
At each cell, four actions are possible: north, south, east, and west, which
deterministically cause the agent to move one cell in the respective direction
on the grid. Actions that would take the agent off the grid leave its location
unchanged, but also result in a reward of -1. In other words, rewards are positive
for goals, negative for running into undesirable states, and zero the rest of the time.

image::../img/vfi_gridworld.png[Gridworld,200,100]

Next we define the rewards for the states. Those will be of +1 for the state
that is desirable, of -1 for states that have to be avoided and of 0 for all
other states.

The Bellman equation (see below) must hold for each state for the value function
stem:[v_{pi}].

stem:[ v_{pi}(s) = \sum_a \pi(a|s) \sum_{s'|r} p(s',r|s,a)  [ r + \gamma v_{\pi}(s') ]  ]
, for all stem:[s \in S]

where the actions, stem:[a], are taken from the set stem:[A(s)], that the next
states, stem:[s'], are taken from the set stem:[S], and that the rewards,
stem:[r], are taken from the set stem:[R].

The Bellman equation expresses a relationship between the value of a state
and the values of its successor states.

Suppose the agent selects all four actions with equal probability in all states.
The figure below shows the value function, stem:[v_{pi}], for this policy, for the
discounted reward case with stem:[\gamma = 0.9].

image::../img/vfi_gridworld_solution.png[Gridworld,200,100]

This value function was computed by solving the system of linear equations in
the expression for the Bellman equation above.

image::../img/vfi_gridworld_arrows.png[Gridworld,200,100]


== Sources:

* link:dynamicprogramming/vfi-robot.py[Source code]
* link:../latex/main-vfi-gridworld.tex[LaTeX source code for the graphics]
* Sutton and Barto. Reinforcement Learning
